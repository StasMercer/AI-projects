{"cells":[{"metadata":{"id":"bhrV98ildJZn","trusted":true},"cell_type":"code","source":"from enum import unique\nimport json\nimport re\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding, GRU\nfrom keras.utils import np_utils\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport os","execution_count":169,"outputs":[]},{"metadata":{"id":"Ar70UAfddJZ1","trusted":true},"cell_type":"code","source":"with open(\"../input/russian-poems2/classic_poems.json\", \"r\", encoding='utf8') as read_file:\n    data = json.load(read_file)\n\npoems = []\nfor obj in data:\n    poems.append(obj['content'])\n\ntext = ''.join(poems)\n\n","execution_count":170,"outputs":[]},{"metadata":{"tags":[],"id":"vk4F3HOJdJaB","outputId":"9fe86a07-070f-4be3-84f9-69f947ee4f62","trusted":true},"cell_type":"code","source":"chars = sorted(set(text))\n\nchars2idx = {c:i for i, c in enumerate(chars)}\nidx2char = np.array(chars)\n\ninput_len = len(text)\nvocab_len = len(chars)\nprint(f\"{input_len} inp, {vocab_len} vocab\")\n\n# I started to use tf dataset to simplify the process\n# First it got a all text as list of numbers\ninput_dataset = tf.data.Dataset.from_tensor_slices([chars2idx[i] for i in text])\nseq_length = 300\n\n# Second, I made batches of seq_length \nsequences = input_dataset.batch(seq_length+1, drop_remainder = True)\n\n# now it holds tuples ((100), (100)) where first is x, second is y\ndef split_input_target(chunk):\n  input_text = chunk[:-1]\n  target_text = chunk[1:]\n  return input_text, target_text\ndataset = sequences.map(split_input_target)\n","execution_count":171,"outputs":[{"output_type":"stream","text":"1968220 inp, 176 vocab\n","name":"stdout"}]},{"metadata":{"id":"z3L-ICwSdJaT","trusted":true},"cell_type":"code","source":"# shuffle and split to batches again\nbatch_size = 32\nembedding_dim = 200\nrnn_units = 1000\n\ndataset = dataset.shuffle(1000).batch(batch_size, drop_remainder=True)","execution_count":172,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n                              batch_input_shape=[batch_size, None]),\n        tf.keras.layers.LSTM(rnn_units,\n                        return_sequences=True,\n                        stateful=True),\n    tf.keras.layers.Dense(vocab_size)\n    ])\n    return model","execution_count":173,"outputs":[]},{"metadata":{"id":"vJdlK7r2dJal","outputId":"d5b8b71e-0a92-437a-9595-ee48ef88385b","trusted":true},"cell_type":"code","source":"model = build_model(vocab_len, embedding_dim, rnn_units, batch_size)\ndef loss(labels, logits):\n    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\nmodel.compile(optimizer='adam', loss=loss)","execution_count":174,"outputs":[]},{"metadata":{"tags":[],"id":"AjJyyNeXdJa0","outputId":"f734d4d4-3dc0-4765-9ad2-c09e97637801","trusted":true},"cell_type":"code","source":"# Directory where the checkpoints will be saved\ncheckpoint_dir = './training_checkpoints'\n# Name of the checkpoint files\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n\ncheckpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True)\n\nmodel.fit(dataset, epochs=50, callbacks=[checkpoint_callback])\nmodel.save('model1')","execution_count":175,"outputs":[{"output_type":"stream","text":"Epoch 1/50\n204/204 [==============================] - 22s 107ms/step - loss: 3.0143\nEpoch 2/50\n204/204 [==============================] - 22s 108ms/step - loss: 2.4922\nEpoch 3/50\n204/204 [==============================] - 22s 108ms/step - loss: 2.3294\nEpoch 4/50\n204/204 [==============================] - 22s 108ms/step - loss: 2.1981\nEpoch 5/50\n204/204 [==============================] - 22s 107ms/step - loss: 2.0866\nEpoch 6/50\n204/204 [==============================] - 22s 108ms/step - loss: 2.0010\nEpoch 7/50\n204/204 [==============================] - 22s 108ms/step - loss: 1.9336\nEpoch 8/50\n204/204 [==============================] - 22s 109ms/step - loss: 1.8788\nEpoch 9/50\n204/204 [==============================] - 22s 108ms/step - loss: 1.8345\nEpoch 10/50\n204/204 [==============================] - 22s 108ms/step - loss: 1.7953\nEpoch 11/50\n204/204 [==============================] - 22s 108ms/step - loss: 1.7623\nEpoch 12/50\n204/204 [==============================] - 22s 107ms/step - loss: 1.7323\nEpoch 13/50\n204/204 [==============================] - 22s 108ms/step - loss: 1.7062\nEpoch 14/50\n204/204 [==============================] - 22s 108ms/step - loss: 1.6806\nEpoch 15/50\n204/204 [==============================] - 22s 107ms/step - loss: 1.6575\nEpoch 16/50\n204/204 [==============================] - 22s 108ms/step - loss: 1.6360\nEpoch 17/50\n204/204 [==============================] - 22s 108ms/step - loss: 1.6162\nEpoch 18/50\n204/204 [==============================] - 22s 107ms/step - loss: 1.5958\nEpoch 19/50\n204/204 [==============================] - 22s 108ms/step - loss: 1.5773\nEpoch 20/50\n204/204 [==============================] - 22s 108ms/step - loss: 1.5586\nEpoch 21/50\n204/204 [==============================] - 22s 108ms/step - loss: 1.5406\nEpoch 22/50\n204/204 [==============================] - 22s 108ms/step - loss: 1.5226\nEpoch 23/50\n204/204 [==============================] - 22s 108ms/step - loss: 1.5041\nEpoch 24/50\n204/204 [==============================] - 22s 109ms/step - loss: 1.4871\nEpoch 25/50\n204/204 [==============================] - 22s 108ms/step - loss: 1.4683\nEpoch 26/50\n204/204 [==============================] - 22s 107ms/step - loss: 1.4505\nEpoch 27/50\n204/204 [==============================] - 22s 108ms/step - loss: 1.4318\nEpoch 28/50\n204/204 [==============================] - 22s 108ms/step - loss: 1.4134\nEpoch 29/50\n204/204 [==============================] - 22s 108ms/step - loss: 1.3946\nEpoch 30/50\n204/204 [==============================] - 22s 109ms/step - loss: 1.3757\nEpoch 31/50\n204/204 [==============================] - 22s 108ms/step - loss: 1.3565\nEpoch 32/50\n204/204 [==============================] - 22s 108ms/step - loss: 1.3378\nEpoch 33/50\n204/204 [==============================] - 22s 108ms/step - loss: 1.3181\nEpoch 34/50\n204/204 [==============================] - 22s 108ms/step - loss: 1.2984\nEpoch 35/50\n204/204 [==============================] - 22s 109ms/step - loss: 1.2794\nEpoch 36/50\n204/204 [==============================] - 22s 107ms/step - loss: 1.2592\nEpoch 37/50\n204/204 [==============================] - 22s 108ms/step - loss: 1.2392\nEpoch 38/50\n204/204 [==============================] - 22s 109ms/step - loss: 1.2201\nEpoch 39/50\n204/204 [==============================] - 22s 108ms/step - loss: 1.2006\nEpoch 40/50\n204/204 [==============================] - 22s 108ms/step - loss: 1.1811\nEpoch 41/50\n204/204 [==============================] - 22s 108ms/step - loss: 1.1626\nEpoch 42/50\n204/204 [==============================] - 22s 107ms/step - loss: 1.1436\nEpoch 43/50\n204/204 [==============================] - 22s 109ms/step - loss: 1.1258\nEpoch 44/50\n204/204 [==============================] - 22s 107ms/step - loss: 1.1068\nEpoch 45/50\n204/204 [==============================] - 22s 108ms/step - loss: 1.0890\nEpoch 46/50\n204/204 [==============================] - 22s 109ms/step - loss: 1.0704\nEpoch 47/50\n204/204 [==============================] - 22s 108ms/step - loss: 1.0541\nEpoch 48/50\n204/204 [==============================] - 22s 108ms/step - loss: 1.0375\nEpoch 49/50\n204/204 [==============================] - 22s 109ms/step - loss: 1.0215\nEpoch 50/50\n204/204 [==============================] - 22s 108ms/step - loss: 1.0052\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model(vocab_len, embedding_dim, rnn_units, batch_size=1)\n\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n\nmodel.build(tf.TensorShape([1, None]))\nmodel.save('/model1')","execution_count":176,"outputs":[]},{"metadata":{"id":"P2mvI-agdJa6","trusted":true},"cell_type":"code","source":"\ndef generate_text(model, start_string):\n  # Evaluation step (generating text using the learned model)\n\n  # Number of characters to generate\n    num_generate = 300\n\n  # Converting our start string to numbers (vectorizing)\n    input_eval = [chars2idx[s] for s in start_string]\n    input_eval = tf.expand_dims(input_eval, 0)\n    print(input_eval.shape)\n  # Empty string to store our results\n    text_generated = []\n\n  # Low temperatures results in more predictable text.\n  # Higher temperatures results in more surprising text.\n  # Experiment to find the best setting.\n    temperature = 0.1\n\n  # Here batch size == 1\n    model.reset_states()\n    for i in range(num_generate):\n        predictions = model(input_eval)\n    # remove the batch dimension\n        predictions = tf.squeeze(predictions, 0)\n\n    # using a categorical distribution to predict the character returned by the model\n        predictions = predictions / temperature\n        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n    # We pass the predicted character as the next input to the model\n    # along with the previous hidden state\n        input_eval = tf.expand_dims([predicted_id], 0)\n\n        text_generated.append(idx2char[predicted_id])\n\n    return (start_string + ''.join(text_generated))","execution_count":181,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(generate_text(model, start_string=u\"Дивный \"))","execution_count":185,"outputs":[{"output_type":"stream","text":"(1, 7)\nДивный из нас,\nУже не вырваться с ней?\nКогда под забором в дом\nТам – в сердце – небесный свет,\nНо старинные объятия,\nНо в отдых открытых видений\nВстречал ее в очи глянулся,\nИ оба покрыл от скуки\nОткрыли дня своей дороги\nНе в силах страсти и в стихах.\nЖизнь в сердце – не находя чужи,\nНад ним собаки не надо.\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}